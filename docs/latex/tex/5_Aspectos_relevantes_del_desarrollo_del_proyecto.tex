\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

\section{Inicio del proyecto}\label{inicio-del-proyecto}

Este proyecto surge con el afán de aprender más sobre la obtención, el almacenamiento, el tratamiento y el análisis de datos financieros. Inicialmente me planteé otras posibilidades que podían cubrir algunas de estas ideas de manejo de información, como el análisis de datos climatológicos, pero la obtención de la información no era sencilla y, casi al mismo tiempo, descubrí que era posible conseguir los datos de valores cotizados a través de diferentes \emph{APIs}\citep{wiki:api} y que podría trabajar con ellos en mi entorno de trabajo local.   

Por otro lado, había leído sobre la aplicación de modelos estadísticos para estimar posibles tendencias y quería comprobar el recorrido que tendrían esas estrategias. Tal vez, de fondo estuviera esa idea ingenua de que algún día nos haremos ricos con algún tipo de algoritmo mágico; pero lo cierto es que algunas de las técnicas empleadas en este trabajo han resultado interesantes y pueden ayudar a formar una cartera bien diversificada, así como a propiciar inversiones con menor riesgo del necesario. Incluso el enfoque de trading algorítmico planteado en este trabajo puede ser útil para realizar pronósticos a muy corto plazo - se realizan estimaciones de un día -.

Finalmente, al plantear mis ideas al Dr. José Ignacio Santos Martín y a la Dra. Virginia Ahedo García, ya me avisaron de que había alguna de mis ideas, como la aplicación de modelos ARIMA, que no iban a funcionar y así ha sido. Pero quería aprovechar la oportunidad para ofrecer herramientas que ayudaran a los usuarios a entender por qué este modelo no funciona, sobre todo basándome en el manejo de los datos, y qué podemos hacer los inversores para utilizar la información a nuestro alcance con otros modelos o métodos de inversión. 

\section{Metodologías}\label{inicio-del-proyecto}

De forma global se ha intentado seguir una metodología ágil a lo largo de todo el proyecto, concretamente, \emph{Scrum}. El inconveniente más evidente de seguir esta filosofía en un proyecto educativo es la falta de un equipo personas que cubran los diferentes roles necesarios. Sin embargo, he intentado ponerme en el papel de cada componente del equipo, haciéndome preguntas constantemente sobre el tiempo disponible, el producto final requerido en cada sprint, qué esperaría un potencial cliente y cómo se comprobaría el código en un entorno laboral. 

Algunos de los procesos más relevantes llevados a cabo han sido:

\begin{itemize}
\tightlist
\item  
Realización de iteraciones, \emph{sprints}, de forma constante y con diferentes temporalidades dependiendo del producto esperado al final de cada periodo. Aquí he intentado realizar un esfuerzo extra en cuanto a lo esperable en las reuniones diarias y, aunque han sido discusiones conmigo mismo, he de reconocer que el resultado en general es satisfactorio, porque me ha ayudado a plantearme diferentes cuestiones que han contribuido a mejorar mi manera de trabajar y que, desde mi punto de vista, han mejorado los incrementos al final de cada \emph{sprint}. 
\item
Disposición de tareas, conocidas como \emph{Issues}\citep{wiki:issue}, asignadas como si se tratara de un proyecto con un equipo multidisciplinar. Estas tareas han sido creadas con un gestor de proyectos llamado \emph{Zube}, que ha facilitado la consecución de objetivos y que ofrece una elevada integración con repositorios de \emph{GitHub}, lo que cual evita tener que desplegar \emph{issues} en diferentes entornos. 
\item
Utilización de un panel \emph{Kanban} integrado en \emph{Zube}. Este tablero de tarjetas ha facilitado el seguimiento de las tareas pendientes. Además, es una forma visual rápida de detectar todo el trabajo que queda por hacer, el tiempo disponible para ello y ayuda a centrar los esfuerzos en las \emph{issues} más urgentes. 
\item
Implementación de un flujo de trabajo ágil, dirigido por las tareas dispuestas en el panel \emph{Kanban}, con diferentes estados para las mismas: \emph{inbox, backlog, ready, in progress, in review} y \emph{done}. Los estados más utilizados han sido los \emph{inbox} para nuevas tareas que se me iban ocurriendo según avanzaba el proyecto, la de \emph{ready} con todas las tareas preparadas para ser realizadas y la de \emph{in progress} para tener claro lo que se estaba realizando en cada momento. El resto también han tenido relevancia, pero en menor grado. 
\end{itemize}

Adicionalmente, se han medido la cantidad de trabajo realizado y las tareas pendientes con gráficos \emph{burndown}\citep{wiki:burndown}. Ha sido frecuente no cumplir a la perfección con los plazos esperados durante los \emph{sprint} y esto se ha debido a la incorporación de pequeñas mejoras en los períodos ya definidos - algo que no se debería hacer -. Sin embargo, la visualización de los \emph{burndown} me ha ayudado a dedicar esfuerzos adicionales para llegar a cumplir con prácticamente todos los objetivos al final de cada iteración. 

\subsection{Ensayo y error en fases tempranas del proyecto}

Antes de la realización de este trabajo había utilizado \emph{Python} de forma extensiva, en diferentes asignaturas y en el ámbito personal, sin embargo, nunca había usado \emph{Django} y esto me obligó a realizar múltiples pruebas inicialmente. 

Buena parte del código que implementé inicialmente se basó en ensayo y error, buscando guías de ayuda y utilizando la documentación de \emph{Django}\citep{online:django_doc}. Este proceso de pruebas iniciales me ha favorecido en la fase final del trabajo, porque me ha permitido realizar los últimos \emph{sprints} de forma más eficiente. 

\subsection{Diseño dirigido por pruebas. TDD}

En la medida de lo posible se han intentado desarrollar tests previos a la implementación de código pero, en múltiples ocasiones, no ha sido posible. Esto se ha debido al desconocimiento previo del autor sobre el funcionamiento de \emph{Django} y a que muchas de las pruebas iban dirigidas hacia el entorno \emph{web} y la comprobación de los resultados esperados en los métodos. 

Sin embargo, el mero hecho de haber intentado implantar este tipo de desarrollo me ayudó en fases iniciales a detectar diferentes fallos y a fortalecer la estructura de pruebas que tenía implementada.



\section{Formación}\label{formación}

Este trabajo ha requerido de algunos conocimientos que previamente no se tenían, especialmente en lo referente a \emph{Django}, a la formación de carteras diversificadas y al \emph{trading} algorítmico. Pero ya se tenían conocimientos de modelos estadísticos, de bases de datos y de desarrollo web que han facilitado algunas de las fases del proyecto. 

Las fuentes fundamentales en las que se ha adquirido el conocimiento necesario sobre \emph{Django} han sido: 

\begin{itemize}
\tightlist
\item  
La documentación de \emph{Django}\citep{online:django_doc}. 
\item
Vídeos de \emph{YouTube} de canales especializados: 
\begin{itemize}
\item
Curso de \emph{Django} para principiantes\citep{online:django_fatz}.
\item
\emph{Django full course}, del que se ha obtenido buena información, especialmente para la implemetación de \emph{routers} para el uso de diferentes bases de datos\citep{online:django_full_course}.
\end{itemize}
\item
La documentación de \emph{Jinja}\citep{online:jinja_doc}.
\end{itemize}

En cuanto a la parte más financiera del trabajo, además de las lecturas realizadas durante años previos sobre diferentes métodos de inversión, debo destacar los siguientes - sobre todo por la nueva visión que me han aportado en cuanto a la creación de una cartera bien diversificada y a la implementación de técnicas de \emph{trading} algorítmico -:

\begin{itemize}
\tightlist
\item  
Parte de la teoría de Harry M. Markowitz, especialmente, con su libro \emph{Portfolio selection. Efficient diversification of investments}\citep{book:Portfolio_selection}.
\item 
Curso de \emph{Udemy} de \emph{Financial Engineering and Artificial Intelligence in Python}. Un curso muy completo que no he terminado todavía, pero que tiene información 
especializada y formal que ha contribuido al desarrollo teórico de este proyecto y a la comprensión de varios conceptos. Lo imparte \emph{The Lazy Programmer Team} y es altamente recomendable para cualquier interesado en la materia\citep{online:financial_engineering}. 
\item
Vídeos de \emph{YouTube} de fuentes especializadas: 
\begin{itemize}
\item
Serie de vídeos sobre cómo implementar la frontera eficiente en \emph{Python}\citep{online:efficient_frontier}\footnote{Aunque no se ha seguido esta técnica exactamente, por el propio proceso de obtención de datos, sí que se han conseguido resultados similares.}. 
\item
Clase del Dr. Peter Kempthorne, del MIT, sobre \emph{Portfolio Theory}\citep{online:portfolio_theory_mit}.
\end{itemize}
\item
Artículo \emph{Modern Portfolio Theory with Python} con algunas ideas interesantes\citep{online:mpt_python_medium}.
\end{itemize}

Han habido otra muchas fuentes y recursos consultadas pero, sin duda, las más relevantes son las expuestas en esta sección. 


\section{Obtención y procesamiento de datos}\label{procesado_de_datos}

La obtención de la información se planteó como algo fundamental en las primeras fases del proyecto y, hasta las fases finales se ha mantenido la estrategia que se decidió adoptar del uso de una API. Para conseguir información de mercados financieros hay diferentes APIs, (\emph{Alpha Vantage}, \emph{Finage} e \emph{IEX CLoud}, entre otras) pero muchas de ellas son de pago o permiten un acceso restringido a pocos datos. Sin embargo, hay una API que está bastante extendida que se llama \emph{yfinance}\citep{online:yfinance}. Esta API, probablemente, sea la mejor ahora mismo para obtener datos de valores cotizados de los principales mercados financieros de forma gratuita. Hace uso de la información de \emph{Yahoo Finance} y es muy utilizada en entornos en los que no es necesario un uso intensivo de datos en tiempo real, como es el caso de este trabajo. Su popularidad en \emph{GitHub} es merecida\citep{online:yfinance_github}.

Teniendo claro cómo obtener los datos y que se iban a almacenar en una base de datos SQLite3, decidí crear unos modelos en \emph{Django} (cada valor cotizado tenía su propio modelo y su correspondiente tabla, creados de forma manual) que permitieran almacenar la información necesaria. Inicialmente sólo tenía una base de datos para el índice DJ30 y en ella se mezclaba información sobre otros índices. Pero rápido se detectó que la estructura no era adecuada. Entonces, la división en diferentes bases de datos, con múltiples modelos creados de forma semiautomática, han resultado ser de gran utilidad y han permitido extender a posteriori la información disponible sin demasiadas complicaciones. 

\subsection{Paso 1. Creación de modelos}

Tras tener claro de dónde obtener los datos empecé a trabajar con algunos modelos muy básicos, que se creaban a mano y se migraban a una única base de datos. Esta forma de trabajar no era práctica e invertí una buena cantidad de tiempo en descubrir cómo generar modelos de forma automática a partir de una lista de \emph{tickers}. La idea fue generar listas con los \emph{tickers} disponibles de cada índice, lo que cual es sencillo y, a partir de esas listas ir creando modelos automáticos para diferentes bases de datos. 

La creación de modelos fue uno de los mayores problemas encontrados al inicio del trabajo, sobre todo porque quería que  fuera escalable a más índices y valores cotizados (hasta el punto de que ahora mismo es viable añadir una nueva base de datos en relativamente pocos pasos). La estrategia seguida fue la de crear un modelo base común e ir modificando el nombre de la tabla de manera dinámica. Así, cada valor cotizado tiene su propia tabla en la base de datos de su índice. La forma de conseguirlo fue la siguiente\footnote{Consultar \texttt{Analysis.models.py} para más información.}: 

\begin{verbatim}
Crear una clase base con una serie de atributos (id, precios, moneda, etc.)

Crear un diccionario de clases vacío

Recorrer una lista con todos los tickers de todos los índices:
    Por cada ticker generar una clase dinámica del tipo de la clase base
	Asignar a la clase dinámica el nombre de la tabla como el del ticker    
	Crear una nueva entrada en el diccionario con la nueva clase dinámica

Hacer el diccionario accesible a los scripts de creación de bases de datos
\end{verbatim}

\subsection{Paso 2. Enrutamiento a la base de datos adecuada}

Como se iban a utilizar diferentes bases de datos y tenía que redirigir la información de manera adecuada configuré un \emph{router} siguiendo los pasos indicados en la documentación de \emph{Django}\citep{online:django_routers}\footnote{Se puede consultar más información sobre el multi routing en el código del proyecto, en \texttt{FAT.routers.router\_bases\_datos.py}}

\subsection{Paso 3. Realizar las migraciones}

Para poder migrar la información a las bases de datos es necesario configurar los motores en \emph{Django}:

\begin{verbatim}
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'databases/db.sqlite3',
    },

    'dj30': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'databases/dj30.sqlite3',
    },

    'ibex35': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'databases/ibex35.sqlite3',
    },

    'ftse100': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'databases/ftse100.sqlite3',
    },

    'dax40': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'databases/dax40.sqlite3',
    },
    
}
\end{verbatim}

Tras haber configurado de manera adecuada las bases de datos ya se pueden hacer las migraciones con los comandos \texttt{makemigrations} y \texttt{migrate} que proporciona el \emph{framework}. Esto es especialmente interesante porque nos evita el tener que realizar las migraciones a mano o tener que generar las consultas SQL necesarias. Además, gestiona el guardado de la información. 

\subsection{Paso 4. Almacenamiento de los datos}

Cada índice tiene su propia base de datos y, adicionalmente, hay otra base de datos común a todas ellas, que tiene información sobre los usuarios, sobre las carteras de los usuarios, sobre divisas (para realizar cálculos adecuados en las carteras de los usuarios) y sobre los sectores de los valores (para agilizar las búsquedas y poder realizar comparaciones)\footnote{La estructura de las tablas de las bases de datos puede verse en los anexos de este trabajo}. 

El pseudocódigo para la obtención de la información y el volcado inicial a las bases de datos es el siguiente\footnote{Para ver cómo se rellenan las bases de datos se puede consultar el script \texttt{util.CrearBDs.py}}

\begin{verbatim}

Inicio de la función crear_bds(índice, bd, logger):
    Registrar información en el logger
    Establecer conexión a la base de datos
    Intentar:
        Iniciar transacción con la conexión (asegurando atomicidad)
        Para cada ticker en el índice:
            Obtener datos históricos del stock (stock = yfinance.Ticker(ticker))
            Ajustar formato de fechas según la base de datos
            Agregar columnas relevantes a los datos históricos
            Calcular medias móviles y añadir información de la compañía
            Verificar existencia de la tabla en la base de datos
            Si la tabla existe:
                Guardar los datos en la base de datos
                Registrar éxito en el logger
            Si no:
                Registrar error en el logger
    Capturar excepciones de SQLite y otras
    Finalmente:
        Cerrar conexión a la base de datos
        Registrar finalización del proceso en el logger
    Retornar
                
\end{verbatim}

\section{División por aplicaciones}

Cuando se trabaja con \emph{Django} es habitual implementar distintas aplicaciones que aporten una solución, a cada problema planteado, de forma individual. Esta estructura y la filosofía subyacente en ella me han permitido ir ampliando los recursos disponibles en la web. 

En la primera fase sólo tenía como objetivo almacenar la información en bases de datos, recuperar esa información y mostrarla en una web. Pero poco a poco fui descubriendo el potencial de \emph{Django} y vi que podía añadir nuevas funcionalidades que resultaran útiles. 

Todas esas funcionalidades, que en cierta medida esperaba poder desarrollar desde el principio, tenían que seguir un orden lógico, así que decidí crear mi propia estructura de aplicaciones internas:

\begin{table}[H]
\centering
\begin{tabular}{p{3cm} p{10cm}}
\toprule
\textbf{Aplicación} & \textbf{Funcionalidad} \\
\midrule
	& Aplicación raíz del proyecto \\
FAT & Gestión y configuración de otras aplicaciones \\
    & Control de migraciones de bases de datos \\
    & Control de enrutamiento a bases de datos \\
\midrule
		 & Registro y login de usuarios \\
         & Mostrar datos de componentes de índices bursátiles \\
         & Gráficas interactivas de valores cotizados \\
Analysis & Comparación de un valor con su sector de referencia \\
         & Comparación entre diferentes valores \\
         & Grafo de correlaciones entre valores \\
         & Noticias RSS \\
\midrule
		  & Control de valores en cartera \\
          & Información sobre evolución de una cartera \\
DashBoard & Gráfica de Markowitz y ratio de Sharpe \\
          & Control de valores en seguimiento \\
          & Diagramas sobre distribución en divisas y sectores \\
\midrule
    & Forecasting de series temporales con ARIMA \\
Lab & Trading algorítmico con cruce de medias \\
    & Predicción con estrategias basadas en ML \\
\midrule
News & Noticias de portada \\
     & Control de gráficos de portada \\
\bottomrule
\end{tabular}
\caption{Estructura de aplicaciones}
\label{apps}
\end{table}

Además, existen otras rutas relevantes dentro del proyecto, en las que se encuentran archivos estáticos, \emph{scripts} de utilidad (con fuentes RSS y métodos para manejar los \emph{tickers}), tests y un log que permite controlar, de manera interna, cómo han funcionado los tests\footnote{Ver los directorios \texttt{static}, \texttt{util}, \texttt{tests} y \texttt{log} respectivamente.}. 

Por otro lado se pueden encontrar las carpetas de documentación, \emph{docs}, del trabajo y la ruta con el índice para revisar la cobertura del código, \emph{htmlcov}, generada con la herramienta \texttt{coverage}\footnote{Ver la estructura completa de los directorios en los anexos}.

 

\section{Fuentes de noticias}\label{noticias}

Además de datos de cotizaciones, en la web se pueden consultar diversas noticias tanto en la página principal como en la página general de cada índice. De forma casi experimental, en la página principal de la aplicación se utiliza una API adicional que es \emph{NewsAPI}. Esta fuente de noticias tiene la ventaja de que provee imágenes asociadas a la información y que se puede hacer un proceso de filtrado sobre diferentes campos y en distintos idiomas. Sin embargo, tras múltiples pruebas, he detectado que su información suele centrarse en noticias de la India y EEUU, y que es muy complicado obtener noticias de España. Pero, en cualquier caso, he decidido dejar esta herramienta habilitada para que la página principal resulte más atractiva.

Por otro lado, se hace uso de fuentes RSS con noticias relacionadas a cada índice bursátil. Las fuentes son públicas y los \emph{feeds} se parsean gracias al módulo \texttt{feedparser}\citep{online:feedparser}.


\section{Securizar claves de APIs}\label{noticias}

Cuando se utiliza una API suele necesitarse una clave para conectarse a ella. El problema de estas claves es que no se deben de hacer públicas y, por tanto, hay que protegerlas. Con la API de \texttt{yFinance} no es necesaria una clave, pero con \texttt{NewsAPI} sí. Por su parte \emph{Django}, además, genera su propia clave (denominada \emph{SECRET\_KEY}). 

Sin estas claves no funciona el proyecto, así que tuve que buscar una manera de favorecer el funcionamiento pero sin comprometer la seguridad. Por supuesto, ya había algo desarrollado que me facilitaría el proceso y ese algo era \texttt{dotenv} y las variables de entorno. 

Tanto en el repositorio de \emph{GitHub} como en la web publicada se han eliminado las claves y se han sustituido por variables de entorno que permiten la ejecución del código sin problema. En el repositorio se crean variables de entorno para poder utilizar las \emph{GitHub actions}, concretamente, la que permite comprobar la cobertura del código con \texttt{coverage}\footnote{Ver el código de \texttt{/.github/worflows/coverage.yml} para ampliar información}. Y en la web se han creado variables de entorno con una consola en el servidor. 

Dentro de este proyecto se ha dejado un archivo de ejemplo, \texttt{FAT/.env.example}, para la configuración de las variables de entorno. Cada usuario que descargue el repositorio deberá utilizar sus propias claves, que son fáciles de conseguir en NewsAPI y al al crear nuestro propio proyecto de \emph{Django}\footnote{Ver la guía de usuario de los anexos para mayor información}. Una vez sustituidas las claves de \texttt{FAT/.env.example} sólo es necesario cambiar el nombre de ese mismo archivo a \texttt{FAT/.env} y todo debería de funcionar con normalidad. 



\section{Desarrollo backend y frontend de \emph{DashBoard} de usuario}\label{desarrollo_dashboard}

Una de las fases del trabajo en las que más tiempo se ha invertido ha sido en el desarrollo de una interfaz que permitiera al usuario ver toda la información relativa a su cartera y que pudiera entender, con facilidad, qué posibilidades existían de mejora, haciendo uso de información sobre una gráfica de Markowitz y unas tablas con información sobre el reparto de pesos de los distintos valores cotizados dentro de la propia cartera. 

Inicialmente, la información se mostraba con tablas muy básicas que no permitían entender los resultados obtenidos con claridad. Tras realizar algunas mejoras, como la inclusión de funciones de \emph{JavaScript}, la compresión de los resultados fue mejorando. 

En este apartado he tenido que realizar modificaciones continuamente, porque iba detectando \emph{bugs} o posibles mejoras que resultaran útiles. En una primera aproximación sólo tenía tablas de valores en cartera, posteriormente añadí información sobre valores en seguimiento, luego detecté que se debía mejorar la contabilidad utilizando una única divisa (me decanté por el euro, por motivos evidentes) y así sucesivamente. 

Entre las mejoras más interesantes realizadas a lo largo del tiempo, destacaría los siguientes:

\begin{itemize}
\item
Creación de un \emph{donut} para obtener información agregada sobre los valores en cartera (con cambio a EUR automático para que el usuario tenga una referencia clara). 
\item
Inclusión de un diagrama de barras que permite ver el reparto de pesos de los valores según el sector al que pertenecen. 
\item
Añadir información de valores en el mismo sector que aquellos que el usuario tenga en seguimiento. 
\end{itemize}

Todas estas modificaciones iban en el mismo sentido: tratar de que los usuarios tuvieran una herramienta para buscar los mejores valores con los que diversificar su cartera. 

Finalmente, en las últimas modificaciones conseguí añadir una gráfica de Markowitz junto con el ratio de Sharpe, así como una tabla que permite ver el reparto de pesos actual y lo que sería idóneo según la \emph{Modern Portfolio Theory}. En el backend\footnote{Es recomendable ver el método \texttt{DashBoard.views.mostrar\_markowitz\_frontera\_y\_mejores()} y todos sus métodos auxiliares} de esta gráfica hay una serie de métodos de minimización que son muy interesantes, pero que me costó implementar, especialmente por la inclusión de restricciones adicionales a los problemas (problemas LP y QP, como se ha explicado en la sección de teoría de esta memoria). 

Todos estos cambios implicaron la creación de modelos adicionales para guardar información sobre los valores en cartera y en seguimiento de un usuario - \texttt{StockComprado} y \texttt{StockSeguimiento}\footnote{Ver \texttt{DashBoard.models.py}} - y sobre el cambio de divisas - \texttt{CambioMoneda}\footnote{Ver \texttt{Analysis.models.py}} -. Según iba creando nuevos modelos iba realizando mejoras incrementales sobre la creación de las bases de datos. 



\section{Desarrollo backend y frontend del \emph{Lab}}\label{desarrollo_lab}

\subsection{Implementación de herramientas para modelo ARIMA}

Los primeros pasos para interactuar con modelos ARIMA fueron los más sencillos posibles. Inicialmente sólo intentaba mostrar un informe ARIMA/SARIMAX con \texttt{auto\_arima} de \texttt{pmdarima} he iba realizando comprobaciones de las métricas obtenidas. 

Posteriormente fui añadiendo formularios para que el usuario pudiera interactuar con el modelo en la búsqueda de los mejores parámetros (p, d, q) posibles. En esta fase me tuve que plantear muchas preguntas:

\begin{itemize}
\item
Un usuario experimentado sabe que este modelo no va a funcionar ¿qué puedo ofrecer que le resulte interesante?
\item
¿Una persona que no haya utilizado ARIMA con anterioridad puede manejarlo de alguna manera muy sencilla?
\item
¿Qué le gustaría ver a una persona que sólo va a analizar los datos?
\end{itemize}

Tratando de contestar a todas estas preguntas fui formando conjuntos de datos fácilmente entendibles y gráficas que resumían las estimaciones. Además, decidí añadir unas tablas que complementan la información de las gráficas, mostrando los errores cometidos, para prevenir al usuario final de utilizar este modelo a través de la comparación con una estrategia naíf - contrastar con un modelo que estima que el día siguiente el precio será el mismo que el día actual -.

Por otro lado, intenté desarrollar un método de validación \emph{walk forward anchored} que permitiera hacer comprobaciones diarias. Mi idea se basó en los códigos del PhD Jason Brownlee\citep{online:walk_forward_code}, pero realicé pequeñas modificaciones que me permitían comprobar el acierto en la tendencia, ya que el precio no me interesaba. El pseudocódigo es el siguiente:

\begin{verbatim}
función validación_walk_forward():
    Inicializar modelo, predicciones y aciertos_tendencia

    Dividir datos en conjuntos de entrenamiento y test

    Inicializar conjunto_total

    Para cada paso t en el conjunto de test:
        Ajustar modelo ARIMA con orden y conjunto_total
        Predicción del valor siguiente
        Agregar predicción a predicciones
        Agregar valor real a conjunto_total

        Si t > 0, obtener valor anterior del conjunto de test; 
        si no, del conjunto de entrenamiento
        
        Determinar si la predicción fue correcta según valores
        anterior y real
        
        Registrar la precisión en aciertos_tendencia

    Devolver modelo, aciertos_tendencia y predicciones
\end{verbatim}

Por último, integré las gráficas de las funciones ACF y PACF, que permiten a los usuarios más avanzados extraer los mejores parámetros (p, d, q) por sí mismos. 

\subsection{Implementación de algoritmos de trading}

En este apartado trabajé de forma casi exploratoria, porque no tenia conocimientos previos sobre cómo aplicar de estas técnicas, aunque conocía que se podían implementar.

\subsubsection{Estrategia de cruce de medias}

Inicialmente, generé los datos necesarios para la estrategia de cruce de medias móviles, algo que con \texttt{pandas} es casi trivial:

\begin{verbatim}
# Ejemplo para una MM50
datos['MM50'] = datos["precio_cierre"].rolling(50)
\end{verbatim} 

Posteriormente, generé el algoritmo como se explica en el apartado teórico de esta memoria. Cabe destacar que lo más complicado es entender el porqué de las señales de este algoritmo y cómo se retrasan los valores que marcan si se está invertido, o no, para obtener los resultados deseables. 

El objetivo con la estrategia de cruce de medias era claro: buscar las medias móviles simples que ofrecieran la mayor rentabilidad en un período concreto, para que el usuario dedujera si merecía la pena seguir ese mismo enfoque en sesiones futuras. Es decir, no se trata de predecir un valor futuro, sino de descubrir si una estrategia ha funcionado bien en el pasado y cómo de bueno sería aplicarlo a posteriori. 

De nuevo llegué a un punto en el que necesitaba comparar los resultados para saber cómo de bien se comportaba este método y la idea fue la de comparar contra una estrategia \emph{buy and hold} en la misma ventana temporal en la que se utiliza el cruce de medias. Tras múltiples pruebas, puedo intuir que la estrategia \emph{buy and hold} suele obtener mejores rentabilidades, pero hay cierto nivel de dependencia con las temporalidades y el valor seleccionado\footnote{Los resultados son tan variables que es recomendable que el usuario saque sus propias conclusiones con los resultados mostrados en la web.}.   

\subsubsection{Estrategia basada en \emph{machine learning}}

El enfoque para utilizar esta estrategia es distinto al utilizado para el método de cruce de medias en el sentido de que aquí sí se busca una predicción para la próxima sesión. Es decir, lo que se quería saber desde el principio era si un modelo de regresión o de clasificación podía estimar si la sesión futura sería alcista o bajista, sin importar el precio final. 

Para poder utilizar estos modelos era esencial disponer de la información bien estructurada y que tanto los valores de entrenamiento como los de test tuvieran cierto sentido. Empecé por probar los modelos sobre un sólo valor, pero los resultados no tenían demasiado sentido, así que decidí seguir un enfoque típico de minería de datos, formando un conjunto de valores cotizados como si fueran atributos y una clase a predecir que sería el índice bursátil de referencia de esos mismos valores cotizados. La selección de los valores se podía haber realizado de múltiples formas (por ejemplo, con un \texttt{Pipeline} con elección de atributos a través de un modelo \texttt{RandomForestClassifier()}) pero decidí que lo más adecuado era darle especial relevancia a los valores que tienen mayor peso en los propios índices. Esta decisión se basa en la capitalización de mercado de cada valor y en la capacidad que tienen algunos valores para \emph{mover} por sí solos la cotización de un índice, precisamente, por tener mayor peso en ellos.

En la fase preliminar de este apartado generé pruebas externas a la plataforma web. Extraje información de mis bases de datos y realicé adaptaciones para ver si era viable utilizar estos modelos en términos de tiempos de ejecución. Los resultados no pudieron ser más satisfactorios, porque se tardaba relativamente poco tiempo en generar una salida y mostrar la información deseada. 

Una vez comprobado que los modelos funcionaban de manera adecuada, empecé a integrar estas soluciones en el proyecto. De nuevo, la técnica para la generación del algoritmo se detalla en en la sección teórica de esta memoria, porque resultará más práctico para el lector. Pero se puede destacar - de manera similar al método del cruce de medias - que lo más relevante es entender que se tienen que retrasar los precios de cierre de los índices un día, para que la estimación que hagan los modelos tenga sentido. 

Tras tomar la decisión de qué valores debían representar a cada índice y con los datos ordenados, sólo quedaba aplicar los modelos y mostrar los resultados. En esta ocasión se decide no disponer los resultados con una gráfica, porque no aportan valor añadido a la interpretación - de hecho, podrían llegar a confundir al usuario final -. Así que la presentación de la salida se realiza en una tabla que compara los aciertos de tendencia tanto en entrenamiento como en test y coteja los retornos logarítmicos contra los que se podrían haber obtenido con una estrategia \emph{buy and hold}. 




\section{Desarrollo de formularios para interactuar con el usuario}\label{desarrollo_formularios}

En prácticamente todos los \emph{sprints} he tenido que trabajar con HTML, CSS, Bootstrap y JavaScript. En algunos momentos de forma intensiva para conseguir un resultado visualmente aceptable y entendible. 

En la parte más dinámica de interactuación con el usuario, los formularios, he realizado un esfuerzo importante en cuanto a manejo de datos y control de errores. Todos los formularios son formularios de \emph{Django}, es decir, están en el lado del servidor y no del cliente. Esto permite, entre otras ventajas, realizar validación automática de usuarios y de modelos asociados a ellos. Además, al utilizar formularios de \emph{Django}, se ha mejorado la gestión de posibles fallos a la hora de introducir los datos necesarios. 

La filosofía seguida para el desarrollo de los formularios ha mantenido dos ideas básicas:

\begin{itemize}
\item
El usuario tiene que saber la información que debe introducir y, a ser posible, con una interfaz amigable. 
\item
El usuario no tiene que poder introducir valores erróneos (intencionada o no intencionadamente).
\end{itemize}

En lo relativo al \emph{DashBoard} hay formularios disponibles para agregar nuevos valores en cartera y para añadir nuevos valores en seguimiento. Y en cuanto al \emph{Lab} hay disponibles formularios para interactuar con los modelos ARIMA y para los algoritmos basados en ML. En todos los casos los métodos de control de errores se han diseñado para que sólo se permitan aquellos datos estrictamente correctos y se rechacen, informando al usuario, aquellos datos que no cumplan con las restricciones necesarias\footnote{Ver los métodos \texttt{DashBoard.views.\_hay\_errores()} y \texttt{Lab.views.\_comprobar\_formularios()}}.


\section{Testing, log de tests y algunos estadísticos relevantes}

Desde el inicio del trabajo tenía claro que una de las facetas a cubrir era el testeo de las aplicaciones de forma ordenada. Cada aplicación, así como las utilidades, tienen sus propios tests por separado. Todas las pruebas se han estructurado de la misma manera en la que están distribuidos los directorios de las aplicaciones. Esto permite llevar un mejor control e ir incrementando las pruebas de manera casi natural. 

Hay 250 tests, que dan una cobertura de código casi completa:

\imagen{img_18_tests.png}{Número de tests realizados. Fuente: elaboración propia}{1}

Las comprobaciones se han realizado con \texttt{coverage} y es posible encontrar una serie de plantillas HTML con información adicional. Dentro del directorio \textbf{htmlcov} se puede encontrar un archivo \textbf{index.html} que al ejecutarlo nos abrirá un informe con datos relevante sobre la cobertura de todo el proyecto. Aquí se muestra el resultado final de dicho informe: 

\tablaSmall{Cobertura de código}
{c|c|c|c|c}
{tabla_coverage}
{
\rowcolor{gray!35}
\textbf{Module} & \textbf{statements} & \textbf{missing} & \textbf{excluded} & \textbf{coverage} \\
}
{
Total & 4094 & 7 & 0 & 100\% \\
}

Además, se puede comprobar un \emph{log}, que fui realizando para un mejor control y que guarda un comentario de todas aquellas pruebas que se van pasando. La estructura del \emph{log} se controla con un \emph{logger} y se pueden comprobar estos datos en el directorio \textbf{log}.

Otras métricas relevantes de este trabajo son\footnote{Información extraída con \texttt{pygount} sin contar los archivos estáticos de \emph{Django} ni el directorio del entorno virtual}:

\tablaSmallAjustadaConPosicion{Número de líneas de código \emph{Python}}
{c|c|c|c}
{tabla_lineas_codigo}
{
\textbf{App/Directorio} & \textbf{Archivos} & \textbf{Líneas código} & \textbf{Líneas comentarios} \\
}
{
FAT & 5 & 86 & 135\\
News & 3 & 139 & 104 \\
Analysis & 6 & 5013 & 523 \\
DashBoard & 7 & 521 & 686 \\
Lab & 6 & 906 & 743 \\
util & 6 & 428 & 812 \\
tests & 16 & 2309 & 307 \\
}

\tablaSmallAjustadaConPosicion{Número de líneas de lenguajes de marcas y \emph{JavaScript}}
{c|c|c|c}
{tabla_lineas_html_css_js_jinja}
{
\textbf{App/Directorio} & \textbf{Archivos} & \textbf{Líneas código} & \textbf{Líneas comentarios} \\
}
{
News & 2 & 258 & 23 \\
Analysis & 16 & 449 & 76 \\
DashBoard & 14 & 698 & 73 \\
Lab & 10 & 830 & 77 \\
static/css & 273 & 5 \\
}


